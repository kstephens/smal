
= SMAL - A (S)imple, (M)ark-sweep (AL)locator =

== Overview ==

SMAL is a simple mark-sweep allocator for small objects.  
It is designed to be as minimal, performant and modular as possible.

* It is not a drop-in replacement for <code>malloc()</code> and <code>free()</code>.
* It uses only <code>malloc()</code>, <code>free()</code>, <code>mmap()</code>, and <code>munmap()</code>.
* It has no default collection scheduling policy; users must explicitly start a collection.
* It has no default root marking policy.
* Is neither conservative, nor type-safe; users can configure it for both scenarios.
* It should work in co-operation with any <code>malloc()</code>/<code>free()</code> implementation.
* Most advanced options are supported on Linux and OS X.

It does, however, optionally support:

* Threads.
* Explicit, exact root declarations of globals and stack variables.
* Implicit, conservative root scanning of stack variables.
* Weak references.
* Reference queues.
* Finalizers.
* Remembered Sets for Mostly Unchanging Object. (See below).

== Main API ==

* <code>smal_type_for(size_t object_size, smal_mark_func mark_func, smal_free_func free_func)</code> locates or creates a new <code>smal_type</code> object for objects of a particular size, mark and free functions.
* <code>smal_alloc(smal_type *type)</code> allocates a new object of <code>smal_type</code>.
* <code>smal_mark_ptr(void * ptr)</code> marks a potental pointer.  This is called from within a <code>smal_type</code> <code>mark_func</code>.
* <code>smal_collect()</code> starts a collection.

== Data Structure Overview ==

<code>smal_type</code> represents a user-data type of a fixed size, mark and free callbacks.

<code>smal_page</code> represents an <code>mmap()</code> region of <code>smal_page_size</code> aligned to <code>smal_page_size</code>.

<code>smal_buffer</code> represents the state of objects in a <code>smal_page</code>.  It handles parcelling and bookkeeping for all objects allocated from its <code>smal_page</code>.

== Types ==

<code>smal_type</code> defines the size of objects and other characteristics that will be allocated from one or more <code>smal_buffer</code>s.
<code>mark_func</code> and <code>free_func</code> callback can be registered for each <code>smal_type</code>.

== Page Allocation ==

Each <code>smal_page</code> is allocated from the OS via <code>mmap()</code> and is ensured to be aligned to <code>smal_page_size</code>.
The reasons for this aggressive alignment are discussed below.
This is achieved by attempting to allocate <code>smal_page_size</code> bytes using <code>MAP_ANON</code> <code>mmap()</code>.  
If this buffer not aligned, SMAL <code>munmap()</code>s the attempted buffer and <code>mmap()</code>s <code>smal_page_size * 2</code> bytes and subsequently <code>munmap()</code>s the unaligned portion(s). 

== Buffer Allocation ==

SMAL support two <code>smal_buffer</code> allocation options: 

* Use a portion at the head each page for the page's <code>smal_buffer</code> structure.
* Allocate a <code>smal_buffer</code> structure using <code>malloc()</code> and store a pointer to it in the first word of the page.

The former has better performance, since mapping between <code>smal_page*</code> and <code>smal_buffer*</code>, but bookeeping operations will dirty memory during allocation and collection.  This can cause unnessary copy-on-write effort in forked processes.
The latter avoids page mutations during allocation and collections but has an addtional pointer dereference when mapping <code>smal_page*</code> pointers to <code>smal_buffer*</code> pointers.

== Object Allocation ==

Objects are allocated by <code>smal_alloc(smal_type*)</code>.   Note: objects are allocated by type, not size.

Objects are parceled, on-demand, from a <code>smal_buffer</code>, by incrementing <code>alloc_ptr</code> by the <code>smal_type</code>'s fixed object size, <code>object_size</code>, starting at <code>begin_ptr</code> and terminating at <code>end_ptr</code>.  

== Object Reclaimation ==

Each <code>smal_buffer</code> keeps its own <code>free_list</code>.  
A free bitmap is also maintained to avoid double-free during sweep.
<code>smal_buffers</code> without active objects are <code>munmap</code>ed and returned to the OS after <code>smal_collect()</code>.
Objects can be explicitly freed by <code>smal_free</code>.

== Allocation Scheduling ==

Since SMAL does not compact or relocate objects during collection, it attempts to allocate from the <code>smal_buffer</code> with the least amount of available objects (either unparceled or on the free list), assocated with the requested <code>smal_type</code>.

== Collection Scheduling ==

SAML has no default collection scheduling algorithm or policy.  Users must decide on when to call <code>smal_collect()</code>.

== Memory Alignment ==

=== Benefits ===

The alignment restrictions of <code>smal_buffers</code> ensures very minimal <code>mmap()</code>/OS-level fragmentation.
It also allows for efficient mapping from arbitrary pointers to <code>smal_buffer*</code>, making SMAL
suitable for conservative and non-conservative collection.

=== Invariants ===

Every potential pointer maps to a unique <code>page_id</code>, computed
by dividing the pointer address by <code>smal_page_size</code>.

Since every <code>mmap()</code>'ed page is always aligned to <code>smal_page_size</code>, 
it is trivial to map a potential object pointer to a potential <code>smal_buffer*</code> or it's <code>page_id</code>.

A non-colliding hash table mapping <code>page_id</code>s to <code>smal_buffer*</code>s is maintained.
A lack of an table entry for a given <code>page_id</code>, means that any potential
pointer mapping within the aligned <code>smal_page_size</code> region associated with the <code>page_id</code> 
is not an object pointer allocated from a <code>smal_buffer</code>.

=== Performance Characteristics ===

Because the table is guaranteed to be non-colliding, mapping an arbitrary pointer to a 
<code>smal_buffer</code> can be done in <code>O(1)</code> time, using a divide (or shift), a modulo and a vector index.
This operation takes 6 x86 instructions.

If a <code>smal_buffer*</code> can be found for a potential pointer, the pointer must also be within the <code>smal_buffer</code>'s region between
<code>begin_ptr</code> and <code>alloc_ptr</code> and must be aligned to <code>object_alignment</code>.
This operation takes 8 x86 instructions.

=== Issues ===

* SMAL cannot service allocations larger than <code>smal_page_size - sizeof(smal_buffer) - object_alignment</code> or <code>smal_page_size - sizeof(void*) - object_alignment</code>, depending on the configuration.
* The <code>page_id</code> hash table may become large and sparse when <code>mmap()</code> tends to not allocate <code>smal_buffer</code>s in a mostly-contigious fashion -- other active allocators (e.g.: <code>malloc()</code>) may cause holes in the address space consumed by SMAL.

== Object Mark/Free Bits ==

Each <code>smal_buffer</code> maintains a mark bitmap and free bitmap indexable by every object parceled from the <code>smal_buffer</code>.

Placing mark bits in a separate bitmap reduces memory overhead, esp. for <code>fork()</code>ed processes -- this protects
copy-on-write pages from full mutation due to mark bit manipuations.

An object's mark bitmap index is computed by dividing, the difference between its pointer and its <code>smal_buffer*</code>, by
the size of the objects (<code>object_size</code>) allocated from the <code>smal_buffer</code>.

If an object pointer is known to be allocated and aligned, <code>smal_mark_ptr_exact()</code> can test the mark bit in 25 x86 instructions,
and mark the bit and recurse into the <code>mark_func</code> in another 7 x86 instructions, for a worst case of 32 x86 instructions.
The non-exact version takes approx. 39 x86 instrutions in the worst case.

== Object Free Lists ==

Each <code>smal_buffer</code> maintains a free list.  Sweeping unused objects into the free list will cause page mutations.  There will be an option to only use the free bitmap to find free objects for allocation, at the expensive of allocation speed.

== Mark Queues ==

SMAL supports an optional mark queue to avoid C stack recursion in <code>mark_func</code>.  This is slightly slower than C recursion but will not cause stack overflows for threads with small C stacks.  The mark queue uses <code>malloc()/free()</code>.
  
== Mark-Sweep ==

The algorithms/data structures are as follows:

=== Mark a (potential) object ===

# Map a potental object pointer to a potential <code>page_id</code>.
# Map a potential <code>page_id</code> a potential <code>smal_buffer*</code> using <code>buffer_table</code>.
# Determine if object pointer within <code>smal_buffer</code>'s <code>smal_page</code> region where object allocations took place.
# Determine if object pointer has proper alignment to the head of the object allocation.
# Determine the mark bitmap offset.
# If object is not already marked, 
# Mark the object in the bitmap, and
# Call the <code>smal_type</code>'s <code>mark_func()</code> function to continue marking other objects.

=== Sweep ===

# For each <code>smal_buffer</code>:
# MORE HERE.

== Mostly Unchanging Objects ==

SMAL supports designating a <code>smal_type</code> as containing mostly unchanging objects.  The smal_buffers for these objects are tracked for mutations using a write barrier per buffer and can be scanned for pointers less frequently by keeping a remembered set of references pointing outside itself.  This is functional on Linux and OS X.

== Sweep Frequency ==

Types can be created that will sweep objects only every N collections.  This should only be used for object types that are unlikely to ever become unreferenced during normal collection lifecycles.  This can be used in conjuntion with "Mostly Unchanging Objects" feature.

== Write Barrier ==

SMAL implements a write barrier to determine if a <code>smal_buffer</code> has been mutated since last collection.  This is used to invalidate remembered sets within a buffer.  Write barrier uses <code>mprotect</code>.  Write barrier faults are trapped with POSIX <code>sigaction</code> on Linux and Mach exception ports on OS X.

== Co-operation and Dependency ==

SMAL relies on <code>MAP_ANON</code> <code>mmap()</code>/<code>munmap()</code> and <code>malloc()</code>/<code>free()</code>, 
thus can co-exist with any other <code>mmap()</code>-based <code>malloc()</code>/<code>free()</code> library.
SMAL probably cannot co-exist with allocators that use <code>sbrk()</code>.

== Configuration ==

The user must provide the following functions:

* <code>void smal_collect_inner_before(void *top_of_stack);</code>
* <code>void smal_collect_before_mark();</code>
* <code>void smal_collect_after_mark();</code>
* <code>void smal_collect_before_sweep();</code>
* <code>void smal_collect_after_sweep();</code>
* <code>void smal_collect_mark_roots();</code>

== Roots ==

The user must provide a <code>smal_collect_mark_roots()</code> function.
This allows the user to configure and optimize for different environments:
* single-threaded vs. multi-threaded
* conservative vs. type-safe 
However, SMAL provides a simple, <code>smal_roots</code> package to explicitly declare root pointers on the C stack or in global variables.

== Threads ==

SMAL supports a simple <code>smal_thread</code> abstraction around <code>pthreads</code>.
The <code>smal_roots</code>, finalization and reference packages are thread-aware and thread-safe.  

See include/smal/thread.h for more info.

=== Thread Issues ===

The rest of the SMAL allocator is not completely thread-safe, yet.
SMAL does not yet have portable mechanisms to:

* discover all active threads,
* pause all threads, 
* collect their stacks, roots and registers,
* resume all threads.

The collection should only need to stop the world during marking and should be able to allow allocation in other threads to continue while sweeping objects, at the expense of allocating additional pages until sweeping is complete.  Alternately, in a "single" thread environment, the sweep phase could be executed in a dedicated thread.  These features are not fully implemented.  

=== Thread Performance ===

When enabled with threading support, 
SMAL attempts to use many smaller mutex and read/write lock regions to avoid long or global locks.  
There is a single allocation read/write lock that is held to allow disabling of allocation of each buffer before collection starts.
The multi-thread version is approx. 2 to 3 times slower single-thread version, when running only one thread.

== Object Enumeration ==

SMAL supports global object enumeration (i.e. Ruby ObjectSpace.each_object).

== Weak References and Reference Queues ==

SMAL contains an optional, thread-safe weak reference and reference queue implementation.
See include/smal/reference.h.

== Finalization ==

SMAL contains an optional, thread-safe finalizer implementation.
See include/smal/finalizer.h.

== Licensing ==

MIT License

== TODO ==

* Thread Support:
** Complete thread-safety.
** Thread-safe stop-world.
** Thread-safe stack and register discovery.
** Active thread discovery.
* Concurrent Sweeping.
