
= SMAL - A (S)imple, (M)ark-sweep (AL)locator =

== Overview ==

SMAL is a simple mark-sweep allocator for small objects.  
It is designed to be as minimal, performant and modular as possible.

* It is not a drop-in replacement for <code>malloc()</code> and <code>free()</code>.
* It uses only <code>malloc()</code>, <code>free()</code>, <code>mmap()</code>, and <code>munmap()</code>.
* It has no default collection scheduling policy; users must specify when to attempt collection.
* It has no default root marking policy.
* Is neither conservative, nor type-safe; users can configure it for both scenarios.
* It should work in co-operation with any <code>malloc()</code>/<code>free()</code> implementation.

It does, however, optionally support:

* Threads.
* Explicit, exact root declarations of globals and stack variables.
* Implicit, conservative root scanning of stack variables.
* Weak references.
* Reference queues.
* Finalizers.

== Data Structure Overview ==

<code>smal_type</code> represents a user-data type of a fixed size, mark and free callbacks.

<code>smal_buffer</code> represents a <code>mmap()</code> region of <code>smal_buffer_size</code> aligned to <code>smal_buffer_size</code>.
Each <code>smal_buffer</code> is at the head of an <code>mmap()</code> region.

== Types ==

<code>smal_type</code> defines the size of objects and other characteristics that will be allocated from one or more <code>smal_buffer</code>s.
<code>mark_func</code> and <code>free_func</code> callback can be registered for each <code>smal_type</code>.

== Buffer Allocation ==

Each <code>smal_buffer</code> and its object allocation region are allocated from the OS via <code>mmap()</code> and is ensured to be aligned to <code>smal_buffer_size</code>.
The reasons for this aggressive alignment are discussed below.
This is achieved by attempting to allocate <code>smal_buffer_size</code> bytes using <code>MAP_ANON</code> <code>mmap()</code>.  
If this buffer not aligned, SMAL <code>munmap()</code>s the attempted buffer and <code>mmap()</code>s <code>smal_buffer_size * 2</code> bytes and subsequently <code>munmap()</code>s the unaligned portion. 

== Object Allocation ==

Objects are allocated by <code>smal_alloc(smal_type*)</code>.   Note: objects are allocated by type, not size.

Objects are parceled, on-demand, from a <code>smal_buffer</code>, by incrementing <code>alloc_ptr</code> by the <code>smal_type</code>'s fixed object size, <code>object_size</code>, starting at <code>begin_ptr</code> and terminating at <code>end_ptr</code>.  

== Object Reclaimation ==

Each <code>smal_buffer</code> keeps its own <code>free_list</code>.  
A free bitmap is also maintained to avoid double-free during sweep.
<code>smal_buffers</code> without active objects are <code>munmap</code>ed and returned to the OS after <code>smal_collect()</code>.
Objects can be explicitly freed by <code>smal_free</code>.

== Allocation Scheduling ==

Since SMAL does not compact or relocate objects during collection, it attempts to allocate from the <code>smal_buffer</code> with the least amount of available objects (either unparceled or on the free list), assocated with the requested <code>smal_type</code>.

'''''THIS IS NOT IMPLEMENTED YET'''''.

== Collection Scheduling ==

SAML has no default collection scheduling algorithm or policy.  Users must decide on when to call <code>smal_collect()</code>.

== Memory Alignment ==

=== Benefits ===

The alignment restrictions of <code>smal_buffers</code> ensures very minimal <code>mmap()</code>/OS-level fragmentation.
It also allows for efficient mapping from arbitrary pointers to <code>smal_buffer*</code>, making SMAL
suitable for conservative and non-conservative collection.

=== Invariants ===

Every potential pointer maps to a unique <code>buffer_id</code>, computed
by dividing a <code>smal_buffer*</code> address by <code>smal_buffer_size</code>.

Since the <code>smal_buffer</code> header is always aligned to <code>smal_buffer_size</code>, 
it is trivial to map a potential object pointer to a potential <code>smal_buffer*</code> or it's <code>buffer_id</code>.

A non-colliding hash table mapping <code>buffer_id</code>s to <code>smal_buffer*</code>s is maintained.
A lack of an table entry for a given <code>buffer_id</code>, means that any potential
pointer mapping within the aligned <code>smal_buffer_size</code> region associated with the <code>buffer_id</code> 
is not an object pointer allocated from a <code>smal_buffer</code>.

=== Performance Characteristics ===

Because the table is guaranteed to be non-colliding, mapping an arbitrary pointer to a 
<code>smal_buffer</code> can be done in <code>O(1)</code> time, using a divide (or shift), a modulo and a vector index.
This operation takes 6 x86 instructions.

If a <code>smal_buffer*</code> can be found for a potential pointer, the pointer must also be within the <code>smal_buffer</code>'s region between
<code>begin_ptr</code> and <code>alloc_ptr</code> and must be aligned to <code>object_size</code>.
This operation takes 8 x86 instructions.

=== Issues ===

* SMAL cannot service allocations larger than <code>smal_buffer_size - sizeof(smal_buffer)</code>.
* The <code>buffer_id</code> hash table may become large and sparse when <code>mmap()</code> tends to not allocate <code>smal_buffer</code>s in a mostly-contigious fashion -- other active allocators (e.g.: <code>malloc()</code>) may cause holes in the address space consumed by SMAL.

== Object Mark Bits ==

Each <code>smal_buffer</code> maintains a mark bitmap indexable by every object parceled from the <code>smal_buffer</code>.
Each mark bitmap is transient; it is <code>malloc()</code>ed and <code>free()</code>ed, on-demand, during <code>smal_collect()</code>.

Placing mark bits in a separate bitmap reduces memory overhead, esp. for <code>fork()</code>ed processes -- this protects
copy-on-write pages from full mutation due to mark bit manipuations.

An object's mark bitmap index is computed by dividing, the difference between its pointer and its <code>smal_buffer*</code>, by
the size of the objects (<code>object_size</code>) allocated from the <code>smal_buffer</code>.

If an object pointer is known to be allocated and aligned, <code>smal_mark_ptr_exact()</code> can test the mark bit in 25 x86 instructions,
and mark the bit and recurse into the <code>mark_func</code> in another 7 x86 instructions, for a worst case of 32 x86 instructions.
The non-exact version takes approx. 39 x86 instrutions in the worst case.

== Mark-Sweep ==

The algorithms/data structures are as follows:

=== Mark a (potential) object ===

# Map potential object pointer to a potential <code>smal_buffer*</code> using <code>buffer_table</code>.
# Determine if object pointer within <code>smal_buffer</code> region where object allocations took place.
# Determine if object pointer has proper alignment to the head of the object allocation.
# Determine the mark bitmap offset.
# If object is not already marked, 
# mark the object in the bitmap and
# call the <code>smal_type</code>'s <code>mark_func()</code> function.

=== Sweep ===

# For each <code>smal_buffer</code>:
# MORE HERE.

== Co-operation and Dependency ==

SMAL relies on <code>MAP_ANON</code> <code>mmap()</code>/<code>munmap()</code> and <code>malloc()</code>/<code>free()</code>, 
thus can co-exist with any other <code>mmap()</code>-based <code>malloc()</code>/<code>free()</code> library.
SMAL probably cannot co-exist with allocators that use <code>sbrk()</code>.

== Roots ==

The user must provide a <code>smal_mark_roots()</code> function.
This allows the user to configure and optimize for different environments:
* single-threaded vs. multi-threaded
* conservative vs. type-safe 
However, SMAL provides a simple, <code>smal_roots</code> package to explicitly declare root pointers on the C stack or in global variables.

== Threads ==

SMAL supports a tiny <code>smal_thread</code> abstraction around <code>pthreads</code>.
The <code>smal_roots</code> package is thread-aware and thread-safe.  
The rest of the SMAL allocator is not yet thread-safe.
See include/smal/thread.h.

== Object Enumeration ==

SMAL supports global object enumeration (i.e. Ruby ObjectSpace.each_object).

== Weak References and Reference Queues ==

SMAL contains an optional weak reference and reference queue implementation.
See include/smal/reference.h.

== Finalization ==

SMAL contains an optional finalizer implementation.
See include/smal/finalizer.h.

== Licensing ==

MIT License

== TODO ==

* Full-thread safety.

